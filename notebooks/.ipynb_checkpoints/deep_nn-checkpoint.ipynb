{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep NN Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../../ucl_irdm2017_project2_group1\"))\n",
    "\n",
    "from ltr.data_load import make_rank_data_csv\n",
    "import ltr.utils\n",
    "import ltr.evals \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "import tensorflow as tf \n",
    "# from IPython.core.display import clear_ouptput\n",
    "import time\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "* Load in the data and write to csv's\n",
    "* Take in the features and normalise, so that $x_i \\in [-1,1] $\n",
    "* Remove outlier querys with very high or low associated documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specify the fold from the MSLR-10K dataset you wish to import \n",
    "fpath = '../../input/'\n",
    "fold_no = 1\n",
    "dataset = ['train', 'vali', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load in dataset and get pandas DataFrames\n",
    "train = make_rank_data_csv(fpath, fold_no, 'train')\n",
    "vali = make_rank_data_csv(fpath, fold_no, 'vali')\n",
    "test = make_rank_data_csv(fpath, fold_no, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a dataset of all data for normalisation and reset indices\n",
    "full_data = pd.concat([train,vali,test])\n",
    "full_data.index = range(full_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get list of unique query ids \n",
    "unique_qry = full_data[\"query_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find stats about features in order to normalise\n",
    "mean_params = []\n",
    "max_params = []\n",
    "min_params = []\n",
    "\n",
    "for q_id in unique_qry:\n",
    "    query = full_data[full_data['query_id'] == str(q_id)].drop(['label', 'query_id'], axis=1).astype(float)\n",
    "    average = list(query.mean())\n",
    "    max_values = list(query.max())\n",
    "    min_values = list(query.min())\n",
    "    mean_params.append([q_id] + average)\n",
    "    max_params.append([q_id] + max_values)\n",
    "    min_params.append([q_id] + min_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_cleaned = pd.DataFrame(mean_params)\n",
    "mean_cleaned.columns = [\"query_id\"] + [\"mean_\" + col for col in full_data.columns[2:]]\n",
    "\n",
    "max_cleaned = pd.DataFrame(max_params)\n",
    "max_cleaned.columns = [\"query_id\"] + [\"max_\" + col for col in full_data.columns[2:]]\n",
    "\n",
    "min_cleaned = pd.DataFrame(min_params)\n",
    "min_cleaned.columns = [\"query_id\"] + [\"min_\" + col for col in full_data.columns[2:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising the features by query parititons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amroberts/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:11: RuntimeWarning: invalid value encountered in divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n"
     ]
    }
   ],
   "source": [
    "norm_features = []\n",
    "\n",
    "for index,row in enumerate(full_data.iterrows()):\n",
    "    # Getting Query ID\n",
    "    q_id = row[1]['query_id']\n",
    "    \n",
    "    # Normalisation formula: 2*(x - min)/(max - min) - 1\n",
    "    x = np.array(row[1][2:].astype(float))\n",
    "    min_val = np.array(min_cleaned[min_cleaned[\"query_id\"] == str(q_id)].drop([\"query_id\"],axis=1).astype(float))\n",
    "    max_val = np.array(max_cleaned[max_cleaned[\"query_id\"] == str(q_id)].drop([\"query_id\"],axis=1).astype(float))\n",
    "    \n",
    "    norm_row = (2*(x - min_val)/(max_val - min_val)) - 1\n",
    "\n",
    "    # Nans indicate division by zero, which means max == min, so setting to zero, Naive fix\n",
    "    norm_row[np.isnan(norm_row)] = 0.0\n",
    "    norm_features.append([q_id] + list(norm_row))\n",
    "    if index%10000 == 0:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pandas dataframe from normalised data, adding label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_cleaned = pd.DataFrame(norm_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm_cleaned.insert(0,'label', cleaned['label'])\n",
    "norm_cleaned.insert(0,'Unnamed: 0', cleaned['Unnamed: 0'])\n",
    "norm_cleaned.columns = cleaned.columns\n",
    "del norm_cleaned['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data back into train / validation / test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_train = norm_cleaned.iloc[0:train.shape[0]+1]\n",
    "clean_val = norm_cleaned.iloc[train.shape[0]+1:train.shape[0]+vali.shape[0]+1]\n",
    "clean_test = norm_cleaned.iloc[train.shape[0]+vali.shape[0]+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Getting Filter Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_train_val = norm_cleaned.iloc[0:723412+235259]\n",
    "qid_counts = dict(Counter(list(clean_train_val['query_id'])))\n",
    "filtered_qid_dict = {int(k): v for k, v in qid_counts.items() if v <= 200 and v >= 70}\n",
    "filtered_qids = filtered_qid_dict.keys()\n",
    "\n",
    "clean_train_filtered = clean_train[clean_train['query_id'].isin(filtered_qids)]\n",
    "clean_val_filtered = clean_val[clean_val['query_id'].isin(filtered_qids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_train_filtered.to_csv(\"Data/Full_Deep_Youtube_Data/normalised_mslr_train_filtered_fld1.csv\", index=False)\n",
    "clean_val_filtered.to_csv(\"Data/Full_Deep_Youtube_Data/normalised_mslr_vali_filtered_fld1.csv\", index=False)\n",
    "\n",
    "clean_train.to_csv(\"Data/Full_Deep_Youtube_Data/normalised_mslr_train_fld1.csv\", index=False)\n",
    "clean_val.to_csv(\"Data/Full_Deep_Youtube_Data/normalised_mslr_vali_fld1.csv\", index=False)\n",
    "clean_test.to_csv(\"Data/Full_Deep_Youtube_Data/normalised_mslr_test_fld1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "tf.set_random_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    \"\"\" Weight initialization \"\"\"\n",
    "    weights = tf.random_normal(shape,stddev=0.8)\n",
    "    return tf.Variable(weights)\n",
    "\n",
    "# def unison_shuffled_copies(a, b, c):\n",
    "#     assert len(a) == len(b)\n",
    "#     assert len(b) == len(c)\n",
    "#     p = np.array(np.random.permutation(len(a)))\n",
    "#     return a[p], b[p], c[p]\n",
    "\n",
    "def forwardprop(X, w_1, w_2, w_3, w_4, w_5, biases_h, biases_h1, biases_h2, biases_h3, biases_y):\n",
    "    \"\"\"\n",
    "    Forward-propagation.\n",
    "    IMPORTANT: yhat is not softmax since TensorFlow's softmax_cross_entropy_with_logits() does that internally.\n",
    "    \"\"\"\n",
    "    h   = tf.nn.dropout(tf.nn.relu(tf.matmul(X, w_1) + biases_h), keep_prob)  # The \\sigma function\n",
    "    h1 = tf.nn.dropout(tf.nn.relu(tf.matmul(h,w_2) + biases_h1), keep_prob)\n",
    "    h2 = tf.nn.dropout(tf.nn.relu(tf.matmul(h1,w_3) + biases_h2), keep_prob)\n",
    "    h3 = tf.nn.dropout(tf.nn.relu(tf.matmul(h2,w_4) + biases_h3), keep_prob)\n",
    "    #h4 = tf.nn.dropout(tf.nn.relu(tf.matmul(h3,w_5) + biases_h4), keep_prob)\n",
    "    \n",
    "    #yhat = tf.matmul(h2, w_4) + biases_y\n",
    "    yhat = tf.matmul(h3, w_5) + biases_y  # The \\varphi function\n",
    "    return yhat\n",
    "\n",
    "def get_doc_data():\n",
    "    train_data = pd.read_csv(\"Data/Full_Deep_Youtube_Data/normalised_mslr_train_fld1.csv\")\n",
    "    train_data = train_data.dropna()\n",
    "    \n",
    "    validation_data = pd.read_csv(\"Data/Full_Deep_Youtube_Data/normalised_mslr_vali_fld1.csv\")\n",
    "    validation_data = validation_data.dropna()\n",
    "    \n",
    "    data = pd.concat([train_data,validation_data])\n",
    "    \n",
    "    zero_label = data[data[\"label\"]==0].iloc[0:5255]\n",
    "    one_label = data[data[\"label\"]==1].iloc[0:5255]\n",
    "    two_label = data[data[\"label\"]==2].iloc[0:5255]\n",
    "    three_label = data[data[\"label\"]==3].iloc[0:5255]\n",
    "    four_label = data[data[\"label\"]==4].iloc[0:5255]\n",
    "    data = pd.concat([zero_label,one_label,two_label,three_label,four_label]).sample(frac=1, random_state=0)\n",
    "    all_X = data[data.columns.difference([\"label\",\"query_id\"])]\n",
    "    all_Y = data[[\"label\",\"query_id\"]]\n",
    "    num_classes = len([int(i) for i in list(all_Y[\"label\"].unique())])\n",
    "\n",
    "    #vec_Y = all_Y.apply(lambda x: [0 if x != label else 1 for label in range(num_classes)])\n",
    "    vec_Y = []\n",
    "    for index,label in all_Y[\"label\"].iteritems():\n",
    "        vec_Y.append([0 if int(label) != element else 1 for element in range(num_classes)])\n",
    "    vec_qid = np.array(all_Y[\"query_id\"])\n",
    "    vec_X = np.array(all_X)\n",
    "    return vec_X, vec_Y, vec_qid\n",
    "\n",
    "def get_test_data():\n",
    "    test_full_data = pd.read_csv(\"Data/Full_Deep_Youtube_Data/normalised_mslr_test_fld1.csv\")\n",
    "    data = test_full_data.dropna()\n",
    "    \n",
    "    all_X = data[data.columns.difference([\"label\",\"query_id\"])]\n",
    "    all_Y = data[[\"label\",\"query_id\"]]\n",
    "    num_classes = len([int(i) for i in list(all_Y[\"label\"].unique())])\n",
    "\n",
    "    #vec_Y = all_Y.apply(lambda x: [0 if x != label else 1 for label in range(num_classes)])\n",
    "    vec_Y = []\n",
    "    for index,label in all_Y[\"label\"].iteritems():\n",
    "        vec_Y.append([0 if int(label) != element else 1 for element in range(num_classes)])\n",
    "    vec_qid = np.array(all_Y[\"query_id\"])\n",
    "    vec_X = np.array(all_X)\n",
    "    \n",
    "    return vec_X, vec_Y, vec_qid\n",
    "\n",
    "def adaptiveLearningRate(n, err_tmin1, err_t, rep):\n",
    "    # Set parameters for adapting learning rate. Sets threshholds\n",
    "    n_inc_ratio = 1\n",
    "    n_inc = 1.005\n",
    "    n_max = 1e3\n",
    "    n_dec_ratio = 1.05\n",
    "    n_dec = 0.3\n",
    "    n_min = 1e-6\n",
    "    rep_max = 10\n",
    "    \n",
    "    # Finds of previous and current error\n",
    "    R = err_tmin1 / err_t\n",
    "    print('R: ', R)\n",
    "\n",
    "    # update weights\n",
    "    # new error if lower, learning rate increased\n",
    "    # new error is greater, learning rate decreased\n",
    "    if R < n_inc_ratio:\n",
    "        if n < n_max:\n",
    "            n = min(n_max, n*n_inc)\n",
    "    elif R > n_dec_ratio:\n",
    "        if n > n_min:\n",
    "            if rep < rep_max:\n",
    "                # RESTORE OLD WEIGHTS\n",
    "                rep += 1\n",
    "                n = max(n_min, n*n_dec)\n",
    "                \n",
    "    return n, rep "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Load in Normalised Values from csv Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec_X,vec_Y,vec_qid  = get_doc_data()\n",
    "test_x, test_y, test_qid = get_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Epoch and btch sizes\n",
    "batch_size = 200\n",
    "num_epochs = 10\n",
    "\n",
    "# Layer's sizes\n",
    "x_size = vec_X.shape[1]   # Number of input nodes: 4 features and 1 bias\n",
    "h_size = 600               # Number of hidden nodes in first hidden layer\n",
    "h1_size = 400                # Number of hidden nodes in first hidden layer\n",
    "h2_size = 200                # Number of hidden nodes in third hidden laye\n",
    "h3_size  = 100\n",
    "#h4_size = 50\n",
    "beta= 0.01\n",
    "y_size = len(vec_Y[0])   # Number of outcomes (3 iris flowers)\n",
    "\n",
    "# Dropout = 1 - keep probability (kp)\n",
    "kp = 0.2\n",
    "\n",
    "# Set adaptive_lr to 1 if you want to use adaptive learning rate \n",
    "adaptive_lr = 0\n",
    "lr = 0.0001\n",
    "\n",
    "# Variable\n",
    "biases_h = tf.Variable(tf.zeros([h_size]))\n",
    "biases_h1 = tf.Variable(tf.zeros([h1_size]))\n",
    "biases_h2 = tf.Variable(tf.zeros([h2_size]))\n",
    "biases_h3 = tf.Variable(tf.random_normal([h3_size]))\n",
    "#biases_h4 = tf.Variable(tf.random_normal([h4_size]))\n",
    "biases_y = tf.Variable(tf.zeros([y_size]))\n",
    "\n",
    "# Symbols\n",
    "X = tf.placeholder(\"float\", shape=[None, x_size])\n",
    "y = tf.placeholder(\"float\", shape=[None, y_size])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "# Weight initializations\n",
    "w_1 = init_weights((x_size, h_size))\n",
    "w_2 = init_weights((h_size, h1_size))\n",
    "w_3 = init_weights((h1_size, h2_size))\n",
    "w_4 = init_weights((h2_size, h3_size))\n",
    "w_5 = init_weights((h3_size, y_size))\n",
    "#w_6 = init_weights((h4_size, y_size))\n",
    "\n",
    "# Forward propagation\n",
    "yhat    = forwardprop(X, w_1, w_2, w_3, w_4, w_5, biases_h, biases_h1, biases_h2, biases_h3, biases_y)\n",
    "confidence = tf.nn.softmax(yhat)\n",
    "predict = tf.argmax(yhat, axis=1)\n",
    "\n",
    "# Backward propagation\n",
    "cost    = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=confidence) +\n",
    "    beta*tf.nn.l2_loss(w_1) + beta*tf.nn.l2_loss(biases_h) +\n",
    "    beta*tf.nn.l2_loss(w_2) + beta*tf.nn.l2_loss(biases_h1) + \n",
    "    beta*tf.nn.l2_loss(w_3) + beta*tf.nn.l2_loss(biases_h2) +\n",
    "    beta*tf.nn.l2_loss(w_4) + beta*tf.nn.l2_loss(biases_y))\n",
    "    \n",
    "# Optimiser     \n",
    "#updates = tf.train.AdamOptimizer(0.01).minimize(cost)\n",
    "updates = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Running of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Initialiasation for adaptive learnign weights\n",
    "rep = 0\n",
    "err_min1 = 0\n",
    "\n",
    "# Creating saver\n",
    "saver = tf.train.Saver()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(0, num_epochs+1):\n",
    "    # New Permutation\n",
    "    temp_vec_X,temp_vec_Y, tempvec_qid = unison_shuffled_copies(np.array(vec_X),np.array(vec_Y), np.array(vec_qid))\n",
    "\n",
    "    # Train with each example\n",
    "    for i in range(len(vec_X)):\n",
    "        sess.run(updates, feed_dict={X: vec_X[i: i + batch_size], y: vec_Y[i: i + batch_size], learning_rate: lr, keep_prob: kp})\n",
    "    \n",
    "    # Get accuracy results for train and test data \n",
    "    train_predictions = sess.run(predict, feed_dict={X: vec_X, y: vec_Y, keep_prob: 1})\n",
    "    train_accuracy = np.mean(np.argmax(vec_Y, axis=1) == train_predictions)\n",
    "    train_pred_composition = collections.Counter(train_predictions)\n",
    "    \n",
    "    test_predictions = sess.run(predict, feed_dict={X: test_x, y: test_y, keep_prob: 1})\n",
    "    test_accuracy  = np.mean(np.argmax(test_y, axis=1) == test_predictions)\n",
    "    test_pred_composition = collections.Counter(test_predictions)\n",
    "\n",
    "    comp = zip(list(np.argmax(test_y, axis=1)),list(test_predictions))\n",
    "    print(comp[0:100])\n",
    "    right = sum([1 if x[0] == 4 and x[1] == 4 else 0 for x in comp])\n",
    "    wrong = sum([1 if (x[0] == 4 or x[1] == 4) and x[0] != x[1] else 0 for x in comp])\n",
    "    \n",
    "    print(\"Epoch = %d, train accuracy = %.2f%%, test accuracy = %.10f%%, 4 Label Accuracy = % .2f%%\"\n",
    "          % (epoch + 1, 100. * train_accuracy, 100. * test_accuracy, 100. * (right/(right+wrong))))\n",
    "\n",
    "    print(\"Training Pred Composition\")\n",
    "    print([(x,train_pred_composition[x]) for x in range(5)])\n",
    "    print(\"Test pred Composition\")\n",
    "    print([(x,test_pred_composition[x]) for x in range(5)])\n",
    "    \n",
    "    # Update Learning Rate\n",
    "    if adaptive_lr == 1:\n",
    "        err = (1 - train_accuracy)\n",
    "        lr, rep = adaptiveLearningRate(lr, err_min1, err, rep)\n",
    "        err_min1 = err\n",
    "        print('error rate: ', err, 'lr: ', lr)\n",
    "    else:\n",
    "        lr = lr\n",
    "    \n",
    "# Save the variables to disk.\n",
    "save_path = saver.save(sess, os.path.join(os.getcwd(), \"model/Deep_MLP_NN.ckpt\"))\n",
    "print(\"Model saved to file\")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-procession\n",
    "* Converting confidence values into predicted labels for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add ops to save and restore all the variables.\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Later, launch the model, use the saver to restore variables from disk, and\n",
    "# do some work with the model.\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"model/Deep_MLP_NN.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    print(predictions)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting confidence levels\n",
    "#confi = list(np.max(raw_output,axis=1))\n",
    "confi = list(np.max(predictions,axis=1))\n",
    "# Getting actually label predictions\n",
    "predictions = [list(x) for x in predictions]\n",
    "pred_labels = [pred.index(max(pred)) for pred in predictions]\n",
    "\n",
    "# Getting target labels\n",
    "true_labels = list(np.argmax(test_y, axis=1))\n",
    "\n",
    "# Get query ID's\n",
    "test_qid = [int(x) for x in test_qid]\n",
    "\n",
    "# Creating dataframe out model outputs\n",
    "model_result = pd.DataFrame({\"Query_ID\": test_qid, \"True_Label\": true_labels,\"Pred_Label\": pred_labels, \"Confidence\": confi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sorting into ranked lists using predicted label and confidence\n",
    "# sorted_predictions = model_result.sort_values([\"Query_ID\", \"Pred_Label\",\"Confidence\"], ascending=False)\n",
    "# sorted_predictions.index = range(sorted_predictions.shape[0])\n",
    "\n",
    "# Saving the results to file\n",
    "# sorted_predictions.to_csv(\"Data/Full_Deep_Youtube_Data/Model_Predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
