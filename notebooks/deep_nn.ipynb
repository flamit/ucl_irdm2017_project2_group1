{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep NN Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../../ucl_irdm2017_project2_group1\"))\n",
    "\n",
    "from ltr.data_load import make_rank_data_csv\n",
    "import ltr.utils\n",
    "import ltr.evals \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from itertools import combinations\n",
    "# from collections import Counter\n",
    "import collections \n",
    "import tensorflow as tf \n",
    "import time\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "* Load in the data and write to csv's\n",
    "* Take in the features and normalise, so that $x_i \\in [-1,1] $\n",
    "* Remove outlier querys with very high or low associated documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Specify the fold from the MSLR-10K dataset you wish to import \n",
    "fpath = '../../input/'\n",
    "fold_no = 1\n",
    "dataset = ['train', 'vali', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load in dataset and get pandas DataFrames\n",
    "train = make_rank_data_csv(fpath, fold_no, 'train')\n",
    "vali = make_rank_data_csv(fpath, fold_no, 'vali')\n",
    "test = make_rank_data_csv(fpath, fold_no, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a dataset of all data for normalisation and reset indices\n",
    "full_data = pd.concat([train,vali,test])\n",
    "full_data.index = range(full_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get list of unique query ids \n",
    "unique_qry = full_data[\"query_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find stats about features in order to normalise\n",
    "mean_params = []\n",
    "max_params = []\n",
    "min_params = []\n",
    "\n",
    "for q_id in unique_qry:\n",
    "    query = full_data[full_data['query_id'] == str(q_id)].drop(['label', 'query_id'], axis=1).astype(float)\n",
    "    average = list(query.mean())\n",
    "    max_values = list(query.max())\n",
    "    min_values = list(query.min())\n",
    "    mean_params.append([q_id] + average)\n",
    "    max_params.append([q_id] + max_values)\n",
    "    min_params.append([q_id] + min_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_cleaned = pd.DataFrame(mean_params)\n",
    "mean_cleaned.columns = [\"query_id\"] + [\"mean_\" + col for col in full_data.columns[2:]]\n",
    "\n",
    "max_cleaned = pd.DataFrame(max_params)\n",
    "max_cleaned.columns = [\"query_id\"] + [\"max_\" + col for col in full_data.columns[2:]]\n",
    "\n",
    "min_cleaned = pd.DataFrame(min_params)\n",
    "min_cleaned.columns = [\"query_id\"] + [\"min_\" + col for col in full_data.columns[2:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalising the features by query parititons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "norm_features = []\n",
    "a = full_data.copy()\n",
    "\n",
    "for index,row in enumerate(a.iterrows()):\n",
    "    # Getting Query ID\n",
    "    q_id = row[1]['query_id']\n",
    "    \n",
    "    # Normalisation formula: 2*(x - min)/(max - min) - 1\n",
    "    x = np.array(row[1][2:].astype(float))\n",
    "    min_val = np.array(min_cleaned[min_cleaned[\"query_id\"] == str(q_id)].drop([\"query_id\"],axis=1).astype(float))\n",
    "    max_val = np.array(max_cleaned[max_cleaned[\"query_id\"] == str(q_id)].drop([\"query_id\"],axis=1).astype(float))\n",
    "    \n",
    "    norm_row = (2*(x - min_val)/(max_val - min_val)) - 1\n",
    "\n",
    "    # Nans indicate division by zero, which means max == min, so setting to zero, Naive fix\n",
    "    norm_row[np.isnan(norm_row)] = 0.0\n",
    "    norm_features.append([q_id] + list(norm_row[0]))\n",
    "    if index%10000 == 0:\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pandas dataframe from normalised data, adding label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = list(full_data.columns)\n",
    "cols.remove('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm_cleaned = pd.DataFrame(norm_features, columns=cols)\n",
    "norm_cleaned.insert(0,'label', full_data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data back into train / validation / test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_train = norm_cleaned.iloc[0:train.shape[0]+1]\n",
    "clean_val = norm_cleaned.iloc[train.shape[0]+1:train.shape[0]+vali.shape[0]+1]\n",
    "clean_test = norm_cleaned.iloc[train.shape[0]+vali.shape[0]+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Getting Filter Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_train_val = norm_cleaned.iloc[0:(train.shape[0]+vali.shape[0]+1)]\n",
    "qid_counts = dict(Counter(list(clean_train_val['query_id'])))\n",
    "filtered_qid_dict = {int(k): v for k, v in qid_counts.items() if v <= 200 and v >= 70}\n",
    "filtered_qids = filtered_qid_dict.keys()\n",
    "\n",
    "clean_train_filtered = clean_train[clean_train['query_id'].isin(filtered_qids)]\n",
    "clean_val_filtered = clean_val[clean_val['query_id'].isin(filtered_qids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpath = '../../output'\n",
    "\n",
    "clean_train_filtered.to_csv(\"{fpath}/normalised_mslr_train_filtered_fld{fold_no}.csv\".format(fpath=fpath,fold_no=fold_no), index=False)\n",
    "clean_val_filtered.to_csv(\"{fpath}/normalised_mslr_vali_filtered_fld{fold_no}.csv\".format(fpath=fpath,fold_no=fold_no), index=False)\n",
    "\n",
    "clean_train.to_csv(\"{fpath}/normalised_mslr_train_fld{fold_no}.csv\".format(fpath=fpath,fold_no=fold_no), index=False)\n",
    "clean_val.to_csv(\"{fpath}/normalised_mslr_vali_fld{fold_no}.csv\".format(fpath=fpath,fold_no=fold_no), index=False)\n",
    "clean_test.to_csv(\"{fpath}/normalised_mslr_test_fld{fold_no}.csv\".format(fpath=fpath,fold_no=fold_no), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "tf.set_random_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    \"\"\" Weight initialization \"\"\"\n",
    "    weights = tf.random_normal(shape,stddev=0.8)\n",
    "    return tf.Variable(weights)\n",
    "\n",
    "def forwardprop(X, w_1, w_2, w_3, w_4, w_5, biases_h, biases_h1, biases_h2, biases_h3, biases_y):\n",
    "    \"\"\"\n",
    "    Forward-propagation.\n",
    "    IMPORTANT: yhat is not softmax since TensorFlow's softmax_cross_entropy_with_logits() does that internally.\n",
    "    \"\"\"\n",
    "    h   = tf.nn.dropout(tf.nn.relu(tf.matmul(X, w_1) + biases_h), keep_prob)  # The \\sigma function\n",
    "    h1 = tf.nn.dropout(tf.nn.relu(tf.matmul(h,w_2) + biases_h1), keep_prob)\n",
    "    h2 = tf.nn.dropout(tf.nn.relu(tf.matmul(h1,w_3) + biases_h2), keep_prob)\n",
    "    h3 = tf.nn.dropout(tf.nn.relu(tf.matmul(h2,w_4) + biases_h3), keep_prob)\n",
    "    \n",
    "    yhat = tf.matmul(h3, w_5) + biases_y  # The \\varphi function\n",
    "    return yhat\n",
    "\n",
    "def unison_shuffled_copies(a, b, c):\n",
    "    assert len(a) == len(b)\n",
    "    assert len(b) == len(c)\n",
    "    p = np.array(np.random.permutation(len(a)))\n",
    "    return a[p], b[p], c[p]\n",
    "\n",
    "def get_doc_data(fpath, fold_no, sample_lim=5255):\n",
    "    train_data = pd.read_csv(\"{fpath}/normalised_mslr_train_fld{fold_no}.csv\".format(fpath=fpath, fold_no=fold_no))\n",
    "    train_data = train_data.dropna()\n",
    "    \n",
    "    validation_data = pd.read_csv(\"{fpath}/normalised_mslr_vali_fld{fold_no}.csv\".format(fpath=fpath, fold_no=fold_no))\n",
    "    validation_data = validation_data.dropna()\n",
    "    \n",
    "    data = pd.concat([train_data,validation_data])\n",
    "    \n",
    "    zero_label = data[data[\"label\"]==0].iloc[0:sample_lim]\n",
    "    one_label = data[data[\"label\"]==1].iloc[0:sample_lim]\n",
    "    two_label = data[data[\"label\"]==2].iloc[0:sample_lim]\n",
    "    three_label = data[data[\"label\"]==3].iloc[0:sample_lim]\n",
    "    four_label = data[data[\"label\"]==4].iloc[0:sample_lim]\n",
    "    data = pd.concat([zero_label,one_label,two_label,three_label,four_label]).sample(frac=1, random_state=0)\n",
    "    all_X = data[data.columns.difference([\"label\",\"query_id\"])]\n",
    "    all_Y = data[[\"label\",\"query_id\"]]\n",
    "    num_classes = len([int(i) for i in list(all_Y[\"label\"].unique())])\n",
    "\n",
    "    vec_Y = []\n",
    "    for index,label in all_Y[\"label\"].iteritems():\n",
    "        vec_Y.append([0 if int(label) != element else 1 for element in range(num_classes)])\n",
    "    vec_qid = np.array(all_Y[\"query_id\"])\n",
    "    vec_X = np.array(all_X)\n",
    "    return vec_X, vec_Y, vec_qid\n",
    "\n",
    "def get_test_data(fpath, fold_no):\n",
    "    test_full_data = pd.read_csv(\"{fpath}/normalised_mslr_test_fld{fold_no}.csv\".format(fold_no=fold_no, fpath=fpath))\n",
    "    data = test_full_data.dropna()\n",
    "    \n",
    "    all_X = data[data.columns.difference([\"label\",\"query_id\"])]\n",
    "    all_Y = data[[\"label\",\"query_id\"]]\n",
    "    num_classes = len([int(i) for i in list(all_Y[\"label\"].unique())])\n",
    "\n",
    "    #vec_Y = all_Y.apply(lambda x: [0 if x != label else 1 for label in range(num_classes)])\n",
    "    vec_Y = []\n",
    "    for index,label in all_Y[\"label\"].iteritems():\n",
    "        vec_Y.append([0 if int(label) != element else 1 for element in range(num_classes)])\n",
    "    vec_qid = np.array(all_Y[\"query_id\"])\n",
    "    vec_X = np.array(all_X)\n",
    "    \n",
    "    return vec_X, vec_Y, vec_qid\n",
    "\n",
    "def adaptiveLearningRate(n, err_tmin1, err_t, rep):\n",
    "    # Set parameters for adapting learning rate. Sets threshholds\n",
    "    n_inc_ratio = 1\n",
    "    n_inc = 1.005\n",
    "    n_max = 1e3\n",
    "    n_dec_ratio = 1.05\n",
    "    n_dec = 0.3\n",
    "    n_min = 1e-6\n",
    "    rep_max = 10\n",
    "    \n",
    "    # Finds of previous and current error\n",
    "    R = err_tmin1 / err_t\n",
    "    print('R: ', R)\n",
    "\n",
    "    # update weights\n",
    "    # new error if lower, learning rate increased\n",
    "    # new error is greater, learning rate decreased\n",
    "    if R < n_inc_ratio:\n",
    "        if n < n_max:\n",
    "            n = min(n_max, n*n_inc)\n",
    "    elif R > n_dec_ratio:\n",
    "        if n > n_min:\n",
    "            if rep < rep_max:\n",
    "                # RESTORE OLD WEIGHTS\n",
    "                rep += 1\n",
    "                n = max(n_min, n*n_dec)\n",
    "                \n",
    "    return n, rep "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Load in Normalised Values from csv Files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpath = '../../output'\n",
    "fold_no = 1\n",
    "\n",
    "vec_X,vec_Y,vec_qid  = get_doc_data(fpath, fold_no)\n",
    "test_x, test_y, test_qid = get_test_data(fpath, fold_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set model parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Epoch and btch sizes\n",
    "batch_size = 200\n",
    "num_epochs = 1\n",
    "\n",
    "# Layer's sizes\n",
    "x_size = vec_X.shape[1]   # Number of input nodes: 4 features and 1 bias\n",
    "h_size = 50               # Number of hidden nodes in first hidden layer\n",
    "h1_size = 40                # Number of hidden nodes in first hidden layer\n",
    "h2_size = 30                # Number of hidden nodes in third hidden laye\n",
    "h3_size  = 20\n",
    "beta= 0.1\n",
    "y_size = len(vec_Y[0])   # Number of outcomes\n",
    "\n",
    "# Dropout = 1 - keep probability (kp)\n",
    "kp = 0.5\n",
    "\n",
    "# Set adaptive_lr to 1 if you want to use adaptive learning rate \n",
    "adaptive_lr = 0\n",
    "lr = 0.0004\n",
    "\n",
    "# Variable\n",
    "biases_h = tf.Variable(tf.zeros([h_size]))\n",
    "biases_h1 = tf.Variable(tf.zeros([h1_size]))\n",
    "biases_h2 = tf.Variable(tf.zeros([h2_size]))\n",
    "biases_h3 = tf.Variable(tf.random_normal([h3_size]))\n",
    "biases_y = tf.Variable(tf.zeros([y_size]))\n",
    "\n",
    "# Symbols\n",
    "X = tf.placeholder(\"float\", shape=[None, x_size])\n",
    "y = tf.placeholder(\"float\", shape=[None, y_size])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "# Weight initializations\n",
    "w_1 = init_weights((x_size, h_size))\n",
    "w_2 = init_weights((h_size, h1_size))\n",
    "w_3 = init_weights((h1_size, h2_size))\n",
    "w_4 = init_weights((h2_size, h3_size))\n",
    "w_5 = init_weights((h3_size, y_size))\n",
    "\n",
    "# Forward propagation\n",
    "yhat    = forwardprop(X, w_1, w_2, w_3, w_4, w_5, biases_h, biases_h1, biases_h2, biases_h3, biases_y)\n",
    "confidence = tf.nn.softmax(yhat)\n",
    "predict = tf.argmax(yhat, axis=1)\n",
    "\n",
    "# Backward propagation\n",
    "cost    = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=confidence) +\n",
    "    beta*tf.nn.l2_loss(w_1) + beta*tf.nn.l2_loss(biases_h) +\n",
    "    beta*tf.nn.l2_loss(w_2) + beta*tf.nn.l2_loss(biases_h1) + \n",
    "    beta*tf.nn.l2_loss(w_3) + beta*tf.nn.l2_loss(biases_h2) +\n",
    "    beta*tf.nn.l2_loss(w_4) + beta*tf.nn.l2_loss(biases_h3) +\n",
    "    beta*tf.nn.l2_loss(w_5) + beta*tf.nn.l2_loss(biases_y))\n",
    "    \n",
    "# Optimiser     \n",
    "updates = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Running of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 3), (3, 3), (1, 3), (0, 3), (0, 3), (1, 3), (0, 4), (0, 4), (2, 4), (1, 1), (1, 3), (2, 1), (2, 4), (1, 4), (2, 4), (1, 1), (2, 4), (0, 4), (1, 3), (0, 3), (1, 1), (0, 4), (0, 4), (1, 4), (0, 4), (0, 3), (2, 4), (2, 3), (2, 4), (1, 4), (1, 4), (0, 4), (0, 4), (1, 4), (0, 3), (0, 1), (1, 1), (3, 4), (0, 3), (1, 4), (1, 4), (1, 3), (0, 4), (1, 4), (1, 4), (1, 3), (0, 4), (2, 4), (0, 4), (2, 3), (1, 1), (0, 4), (0, 4), (0, 1), (2, 4), (1, 4), (1, 4), (1, 1), (3, 3), (2, 4), (0, 3), (0, 3), (1, 4), (0, 3), (2, 4), (1, 4), (2, 1), (1, 4), (2, 4), (1, 3), (0, 4), (1, 4), (3, 4), (1, 3), (0, 3), (3, 4), (0, 4), (0, 4), (1, 3), (1, 4), (0, 3), (1, 3), (1, 4), (0, 1), (1, 4), (3, 4), (1, 3), (2, 3), (2, 4), (0, 4), (0, 3), (2, 4), (0, 3), (0, 4), (2, 3), (1, 3), (2, 4), (1, 4), (2, 1), (1, 4)]\n",
      "Epoch = 1, train accuracy = 18.26%, test accuracy = 11.2930606161%, 4 Label Accuracy =  0.00%\n",
      "Training Pred Composition\n",
      "[(0, 0), (1, 8543), (2, 2), (3, 5588), (4, 12142)]\n",
      "Test pred Composition\n",
      "[(0, 0), (1, 68760), (2, 36), (3, 53330), (4, 119394)]\n",
      "we are here\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Initialiasation for adaptive learnign weights\n",
    "rep = 0\n",
    "err_min1 = 0\n",
    "\n",
    "# Creating saver\n",
    "saver = tf.train.Saver()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(0, num_epochs):\n",
    "    # New Permutation\n",
    "    temp_vec_X,temp_vec_Y, tempvec_qid = unison_shuffled_copies(np.array(vec_X),np.array(vec_Y), np.array(vec_qid))\n",
    "\n",
    "    # Train with each example\n",
    "    for i in range(len(vec_X)):\n",
    "        sess.run(updates, feed_dict={X: vec_X[i: i + batch_size], y: vec_Y[i: i + batch_size], learning_rate: lr, keep_prob: kp})\n",
    "    \n",
    "    # Get accuracy results for train and test data \n",
    "    train_predictions = sess.run(predict, feed_dict={X: vec_X, y: vec_Y, keep_prob: 1})\n",
    "    train_accuracy = np.mean(np.argmax(vec_Y, axis=1) == train_predictions)\n",
    "    train_pred_composition = collections.Counter(train_predictions)\n",
    "    \n",
    "    test_predictions = sess.run(predict, feed_dict={X: test_x, y: test_y, keep_prob: 1})\n",
    "    test_accuracy  = np.mean(np.argmax(test_y, axis=1) == test_predictions)\n",
    "    test_pred_composition = collections.Counter(test_predictions)\n",
    "\n",
    "    comp = zip(list(np.argmax(test_y, axis=1)),list(test_predictions))\n",
    "    print(comp[0:100])\n",
    "    right = sum([1 if x[0] == 4 and x[1] == 4 else 0 for x in comp])\n",
    "    wrong = sum([1 if (x[0] == 4 or x[1] == 4) and x[0] != x[1] else 0 for x in comp])\n",
    "    \n",
    "    print(\"Epoch = %d, train accuracy = %.2f%%, test accuracy = %.10f%%, 4 Label Accuracy = % .2f%%\"\n",
    "          % (epoch + 1, 100. * train_accuracy, 100. * test_accuracy, 100. * (right/(right+wrong))))\n",
    "\n",
    "    print(\"Training Pred Composition\")\n",
    "    print([(x,train_pred_composition[x]) for x in range(5)])\n",
    "    print(\"Test pred Composition\")\n",
    "    print([(x,test_pred_composition[x]) for x in range(5)])\n",
    "    \n",
    "    # Update Learning Rate\n",
    "    if adaptive_lr == 1:\n",
    "        err = (1 - train_accuracy)\n",
    "        lr, rep = adaptiveLearningRate(lr, err_min1, err, rep)\n",
    "        err_min1 = err\n",
    "        print('error rate: ', err, 'lr: ', lr)\n",
    "    else:\n",
    "        lr = lr\n",
    "    \n",
    "    # Take final run and save predicitions for later analysis\n",
    "    if epoch == num_epochs-1:\n",
    "        print('we are here')\n",
    "        predictions = sess.run(confidence, feed_dict={X: test_x, y: test_y, keep_prob: 1.0})\n",
    "        raw_output = sess.run(yhat, feed_dict={X: test_x, y: test_y, keep_prob: 1.0})\n",
    "        \n",
    "        # Getting confidence levels\n",
    "        confi = list(np.max(predictions,axis=1))\n",
    "\n",
    "        # Getting label predictions\n",
    "        predictions = [list(x) for x in predictions]\n",
    "        pred_labels = [pred.index(max(pred)) for pred in predictions]\n",
    "\n",
    "        # Getting target labels\n",
    "        true_labels = list(np.argmax(test_y, axis=1))\n",
    "\n",
    "        # Get query ID's\n",
    "        test_qid = [int(x) for x in test_qid]\n",
    "\n",
    "        # Creating dataframe out model outputs\n",
    "        model_result = pd.DataFrame({\"qid\": test_qid, \"label_true\": true_labels,\"label_pred\": pred_labels, \"confidence\": confi})\n",
    "        \n",
    "        # Sorting them into rank lists using predicted label and confidence\n",
    "        sorted_predictions = model_result.sort_values([\"qid\", \"label_pred\",\"confidence\"], ascending=False)\n",
    "        sorted_predictions.index = range(sorted_predictions.shape[0])\n",
    "\n",
    "        # Saving the results to file\n",
    "        sorted_predictions.to_csv(\"../../output/Model_Predictions.csv\", index=0) \n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_predictions(model_result):\n",
    "#     Get list of query ids\n",
    "    unique_qid = model_result['qid'].unique()\n",
    "    \n",
    "    # Create data frame to store results for each q_id\n",
    "    columns = ['qid','ndcg', 'err']\n",
    "    score_rec = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    for qid in unique_qid:\n",
    "        # Get list of all documents for a given qid\n",
    "        qid_list = model_result[model_result['qid']==qid]\n",
    "        qid_list = qid_list.sort_values(by=['label_pred', 'confidence'], ascending=[0, 0])\n",
    "\n",
    "        ndcg = ltr.evals.ndcg_at_rank_N(qid_list['label_true'], 10)\n",
    "        err = ltr.evals.err(qid_list['label_true'], 10)\n",
    "        row = [qid, ndcg, err]\n",
    "\n",
    "        score_rec = score_rec.append(pd.Series(row, index=columns), ignore_index=True)\n",
    "    \n",
    "    return score_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_result = pd.read_csv(\"../../output/Model_Predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>ndcg</th>\n",
       "      <th>err</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29998.0</td>\n",
       "      <td>0.322430</td>\n",
       "      <td>0.299049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29983.0</td>\n",
       "      <td>0.126168</td>\n",
       "      <td>0.210681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29968.0</td>\n",
       "      <td>0.144978</td>\n",
       "      <td>0.167099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29953.0</td>\n",
       "      <td>0.271724</td>\n",
       "      <td>0.186647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29938.0</td>\n",
       "      <td>0.100717</td>\n",
       "      <td>0.112075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>29923.0</td>\n",
       "      <td>0.025387</td>\n",
       "      <td>0.097872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>29908.0</td>\n",
       "      <td>0.108498</td>\n",
       "      <td>0.147453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>29893.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>29878.0</td>\n",
       "      <td>0.216891</td>\n",
       "      <td>0.251053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>29863.0</td>\n",
       "      <td>0.157981</td>\n",
       "      <td>0.180079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>29848.0</td>\n",
       "      <td>0.186287</td>\n",
       "      <td>0.207826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>29833.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>29818.0</td>\n",
       "      <td>0.491689</td>\n",
       "      <td>0.562578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>29803.0</td>\n",
       "      <td>0.219100</td>\n",
       "      <td>0.227028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>29788.0</td>\n",
       "      <td>0.393984</td>\n",
       "      <td>0.357824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>29773.0</td>\n",
       "      <td>0.281041</td>\n",
       "      <td>0.204048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>29758.0</td>\n",
       "      <td>0.372111</td>\n",
       "      <td>0.318817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>29743.0</td>\n",
       "      <td>0.123442</td>\n",
       "      <td>0.187948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>29728.0</td>\n",
       "      <td>0.113758</td>\n",
       "      <td>0.366541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>29713.0</td>\n",
       "      <td>0.137352</td>\n",
       "      <td>0.116662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>29698.0</td>\n",
       "      <td>0.011840</td>\n",
       "      <td>0.090308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>29683.0</td>\n",
       "      <td>0.289065</td>\n",
       "      <td>0.099163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>29668.0</td>\n",
       "      <td>0.384162</td>\n",
       "      <td>0.359640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>29653.0</td>\n",
       "      <td>0.069442</td>\n",
       "      <td>0.189409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>29638.0</td>\n",
       "      <td>0.208806</td>\n",
       "      <td>0.234760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>29623.0</td>\n",
       "      <td>0.058370</td>\n",
       "      <td>0.129797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>29608.0</td>\n",
       "      <td>0.356207</td>\n",
       "      <td>0.082101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29593.0</td>\n",
       "      <td>0.375935</td>\n",
       "      <td>0.458389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29578.0</td>\n",
       "      <td>0.294525</td>\n",
       "      <td>0.408233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29563.0</td>\n",
       "      <td>0.159829</td>\n",
       "      <td>0.169360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>448.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>433.0</td>\n",
       "      <td>0.040057</td>\n",
       "      <td>0.144101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>418.0</td>\n",
       "      <td>0.104307</td>\n",
       "      <td>0.171374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>403.0</td>\n",
       "      <td>0.236352</td>\n",
       "      <td>0.200474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>388.0</td>\n",
       "      <td>0.529239</td>\n",
       "      <td>0.297900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>373.0</td>\n",
       "      <td>0.075816</td>\n",
       "      <td>0.133522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>358.0</td>\n",
       "      <td>0.115971</td>\n",
       "      <td>0.128737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>343.0</td>\n",
       "      <td>0.237708</td>\n",
       "      <td>0.127734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>328.0</td>\n",
       "      <td>0.300283</td>\n",
       "      <td>0.185213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>313.0</td>\n",
       "      <td>0.444141</td>\n",
       "      <td>0.200378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>298.0</td>\n",
       "      <td>0.251923</td>\n",
       "      <td>0.347705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>283.0</td>\n",
       "      <td>0.150539</td>\n",
       "      <td>0.198422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>268.0</td>\n",
       "      <td>0.044831</td>\n",
       "      <td>0.102760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1983</th>\n",
       "      <td>253.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1984</th>\n",
       "      <td>238.0</td>\n",
       "      <td>0.286493</td>\n",
       "      <td>0.340570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1985</th>\n",
       "      <td>223.0</td>\n",
       "      <td>0.297272</td>\n",
       "      <td>0.128934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986</th>\n",
       "      <td>208.0</td>\n",
       "      <td>0.049541</td>\n",
       "      <td>0.120472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>193.0</td>\n",
       "      <td>0.064246</td>\n",
       "      <td>0.157634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>178.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>163.0</td>\n",
       "      <td>0.048202</td>\n",
       "      <td>0.106652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>148.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.085073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>133.0</td>\n",
       "      <td>0.198826</td>\n",
       "      <td>0.148998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>118.0</td>\n",
       "      <td>0.030705</td>\n",
       "      <td>0.138132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993</th>\n",
       "      <td>103.0</td>\n",
       "      <td>0.067707</td>\n",
       "      <td>0.160646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>88.0</td>\n",
       "      <td>0.083488</td>\n",
       "      <td>0.120638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>73.0</td>\n",
       "      <td>0.307219</td>\n",
       "      <td>0.505438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>58.0</td>\n",
       "      <td>0.217727</td>\n",
       "      <td>0.507965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>43.0</td>\n",
       "      <td>0.615531</td>\n",
       "      <td>0.977887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>28.0</td>\n",
       "      <td>0.207350</td>\n",
       "      <td>0.200761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>13.0</td>\n",
       "      <td>0.162624</td>\n",
       "      <td>0.207398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          qid      ndcg       err\n",
       "0     29998.0  0.322430  0.299049\n",
       "1     29983.0  0.126168  0.210681\n",
       "2     29968.0  0.144978  0.167099\n",
       "3     29953.0  0.271724  0.186647\n",
       "4     29938.0  0.100717  0.112075\n",
       "5     29923.0  0.025387  0.097872\n",
       "6     29908.0  0.108498  0.147453\n",
       "7     29893.0  0.000000  0.085073\n",
       "8     29878.0  0.216891  0.251053\n",
       "9     29863.0  0.157981  0.180079\n",
       "10    29848.0  0.186287  0.207826\n",
       "11    29833.0  0.000000  0.085073\n",
       "12    29818.0  0.491689  0.562578\n",
       "13    29803.0  0.219100  0.227028\n",
       "14    29788.0  0.393984  0.357824\n",
       "15    29773.0  0.281041  0.204048\n",
       "16    29758.0  0.372111  0.318817\n",
       "17    29743.0  0.123442  0.187948\n",
       "18    29728.0  0.113758  0.366541\n",
       "19    29713.0  0.137352  0.116662\n",
       "20    29698.0  0.011840  0.090308\n",
       "21    29683.0  0.289065  0.099163\n",
       "22    29668.0  0.384162  0.359640\n",
       "23    29653.0  0.069442  0.189409\n",
       "24    29638.0  0.208806  0.234760\n",
       "25    29623.0  0.058370  0.129797\n",
       "26    29608.0  0.356207  0.082101\n",
       "27    29593.0  0.375935  0.458389\n",
       "28    29578.0  0.294525  0.408233\n",
       "29    29563.0  0.159829  0.169360\n",
       "...       ...       ...       ...\n",
       "1970    448.0  0.000000  0.085073\n",
       "1971    433.0  0.040057  0.144101\n",
       "1972    418.0  0.104307  0.171374\n",
       "1973    403.0  0.236352  0.200474\n",
       "1974    388.0  0.529239  0.297900\n",
       "1975    373.0  0.075816  0.133522\n",
       "1976    358.0  0.115971  0.128737\n",
       "1977    343.0  0.237708  0.127734\n",
       "1978    328.0  0.300283  0.185213\n",
       "1979    313.0  0.444141  0.200378\n",
       "1980    298.0  0.251923  0.347705\n",
       "1981    283.0  0.150539  0.198422\n",
       "1982    268.0  0.044831  0.102760\n",
       "1983    253.0  0.000000  0.085073\n",
       "1984    238.0  0.286493  0.340570\n",
       "1985    223.0  0.297272  0.128934\n",
       "1986    208.0  0.049541  0.120472\n",
       "1987    193.0  0.064246  0.157634\n",
       "1988    178.0  0.000000  0.085073\n",
       "1989    163.0  0.048202  0.106652\n",
       "1990    148.0  0.000000  0.085073\n",
       "1991    133.0  0.198826  0.148998\n",
       "1992    118.0  0.030705  0.138132\n",
       "1993    103.0  0.067707  0.160646\n",
       "1994     88.0  0.083488  0.120638\n",
       "1995     73.0  0.307219  0.505438\n",
       "1996     58.0  0.217727  0.507965\n",
       "1997     43.0  0.615531  0.977887\n",
       "1998     28.0  0.207350  0.200761\n",
       "1999     13.0  0.162624  0.207398\n",
       "\n",
       "[2000 rows x 3 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_rec = score_predictions(model_result)\n",
    "score_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ndcg: ', 0.18066535450125656, ' +- ', 0.15397212837600135)\n",
      "('err: ', 0.20427385157148484, ' +- ', 0.12588851062884993)\n"
     ]
    }
   ],
   "source": [
    "ndcg_mean = np.mean(score_rec['ndcg'])\n",
    "ndcg_std = np.std(score_rec['ndcg'])\n",
    "\n",
    "err_mean = np.mean(score_rec['err'])\n",
    "err_std = np.std(score_rec['err'])\n",
    "\n",
    "print('ndcg: ', ndcg_mean, ' +- ', ndcg_std)\n",
    "print('err: ', err_mean, ' +- ', err_std)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
